{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f35ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c68eb0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_breast_cancer()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e30d48c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5423f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((426, 30), (143, 30))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73f85331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import e\n",
    "\n",
    "\"\"\"\n",
    "entropy is a concept used in decision trees and related algorithms to measure the impurity or disorder of a set of instances. \n",
    "The entropy of a set is highest when all instances in the set belong to different classes,\n",
    "and lowest when all instances belong to the same class. \n",
    "The entropy value is used as a metric to determine the best feature to split a set into smaller subsets, \n",
    "with the goal of reducing impurity and improving the accuracy of predictions.\n",
    "\"\"\"\n",
    "\n",
    "def _entropy(y):\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    probas = counts/counts.sum()\n",
    "    size  = -e if len(counts)==1 else np.log(len(counts))\n",
    "    return -np.sum(probas * np.log(probas))/size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e282498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Information gain is a measure used in decision tree algorithms to quantify the reduction in \n",
    "entropy or impurity achieved by partitioning a set of instances based on a specific feature.\n",
    "It is the difference between the entropy of the original set and the weighted average of the entropies\n",
    "of the resulting subsets. The goal is to select the feature with the highest information gain,\n",
    "as this would result in the largest reduction in impurity and the best split. \n",
    "Information gain is used as a criterion for selecting the best feature to split a set \n",
    "at each step of the decision tree building process.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def _entropy(y):\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    probas = counts/counts.sum()\n",
    "    size  = -e if len(counts)==1 else np.log(len(counts))\n",
    "    return -np.sum(probas * np.log(probas))/size\n",
    "\n",
    "\n",
    "def _split(X, feature, threshold):\n",
    "    left_idxs = np.argwhere(X[:, feature]<=threshold).flatten()\n",
    "    right_idxs = np.argwhere(X[:, feature]>threshold).flatten()\n",
    "    return left_idxs, right_idxs\n",
    "\n",
    "\n",
    "def _information_gain(X, y, feature, threshold):\n",
    "    parent = _entropy(y)\n",
    "    left_idxs, right_idxs = _split(X, feature, threshold)\n",
    "\n",
    "    if len(left_idxs)==0 or len(right_idxs)==0:\n",
    "        return 0\n",
    "\n",
    "    l_child = y[left_idxs]\n",
    "    r_child = y[right_idxs]\n",
    "\n",
    "    n_samples = y.shape[0]\n",
    "    l_samples = l_child.shape[0]\n",
    "    r_samples = r_child.shape[0]\n",
    "\n",
    "    l_entropy = _entropy(l_child)\n",
    "    r_entropy = _entropy(r_child)\n",
    "\n",
    "    ig = parent - ((l_samples/n_samples) * l_entropy) + ((l_samples/n_samples) * r_entropy)\n",
    "\n",
    "    return ig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16fb2d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The \"best split\" in a decision tree refers to the process of dividing a node \n",
    "(representing a set of instances) into two or more child nodes, \n",
    "based on the values of a specific feature. \n",
    "The goal of this process is to create homogeneous child nodes, \n",
    "with instances belonging to the same class as much as possible. \n",
    "The best split is found by selecting the feature that results in the largest reduction in entropy or impurity,\n",
    "as measured by metrics such as information gain or Gini index. By repeatedly splitting nodes in this way,\n",
    "the decision tree algorithm builds a tree-like structure that can be used for classification or regression tasks.\n",
    "\"\"\"\n",
    "\n",
    "def _determine_best_split(X, y, features):\n",
    "    best_gain=-np.inf\n",
    "    best_feature=None\n",
    "    best_threshold=None\n",
    "    for _, f in enumerate(features):\n",
    "        thresholds = np.unique(X[:,f])\n",
    "        for threshold in thresholds:\n",
    "            ig = _information_gain(X, y, f, threshold)\n",
    "\n",
    "            if ig>best_gain:\n",
    "                best_gain=ig\n",
    "                best_feature=f\n",
    "                best_threshold=threshold\n",
    "    return best_feature, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "af712f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\"\"\"\n",
    "decision tree is a type of algorithm used in machine learning and artificial intelligence \n",
    "for solving problems related to classification and regression. It works by constructing a tree-like model,\n",
    "where each internal node represents a test on a feature, each branch represents an outcome of the test,\n",
    "and each leaf node represents a prediction (in classification) or a value (in regression). \n",
    "\"\"\"\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, value=None, left=None, right=None, feature=None, threshold=None):\n",
    "        self.value=value\n",
    "        self.left=left\n",
    "        self.right=right\n",
    "        self.feature=feature\n",
    "        self.threshold=threshold\n",
    "\n",
    "\n",
    "def build_tree(X, y, counter=1, max_depth=3, min_samples_leaf=50, min_features=None):\n",
    "    min_samples, n_features = X.shape\n",
    "\n",
    "    features = n_features if min_features is None else min_features\n",
    "\n",
    "    labels= np.unique(y)\n",
    "\n",
    "    if counter>=max_depth or len(labels)==1 or min_samples<min_samples_leaf:\n",
    "        leaf_val = Counter(y).most_common(1)[0][0]\n",
    "        return Node(value=leaf_val)\n",
    "\n",
    "    features = np.random.choice(n_features, features, replace=False)\n",
    "\n",
    "    best_feature, best_value = _determine_best_split(X, y, features)\n",
    "\n",
    "    left_idxs, right_idxs = _split(X, best_feature, best_value)\n",
    "\n",
    "    left_child = build_tree(X[left_idxs, :], y[left_idxs], counter+1)\n",
    "\n",
    "    right_child = build_tree(X[right_idxs, :], y[right_idxs], counter+1)\n",
    "\n",
    "    return Node(left=left_child, right=right_child, feature=best_feature, threshold=best_value)\n",
    "\n",
    "\n",
    "\n",
    "def predict(X, tree):\n",
    "    predicted = np.array([_traverse(x, tree) for x in X])\n",
    "    return predicted\n",
    "\n",
    "\n",
    "def _traverse(X, tree):\n",
    "    if tree.value is not None:\n",
    "        return tree.value\n",
    "\n",
    "    if X[tree.feature]<=tree.threshold:\n",
    "        return _traverse(X, tree.left)\n",
    "\n",
    "    return _traverse(X, tree.right)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "76487688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.84 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7902097902097902"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tree = build_tree(X_train, y_train)\n",
    "\n",
    "predictions = predict(X_test, tree)\n",
    "\n",
    "np.sum(predictions==y_test)/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3fa335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bootstrap sampling is a statistical technique used in machine learning\n",
    "and other fields to estimate the uncertainty of a model's predictions.\n",
    " \n",
    "It involves creating multiple samples of the training data by randomly\n",
    "selecting instances with replacement, so that each sample has the same size\n",
    "as the original data but may contain duplicate instances. By training a model on each bootstrapped sample,\n",
    "we obtain a collection of models, each fit to a different random subset of the data.\n",
    "\n",
    "This collection can then be used to obtain various statistical measures,\n",
    "such as the mean and standard deviation of model performance,\n",
    " or to estimate the uncertainty of the model's predictions for new instances. \n",
    "\n",
    "Bootstrap sampling is a widely used technique for estimating the stability \n",
    "and generalization performance of machine learning models.\n",
    "\"\"\"\n",
    "\n",
    "def _bootstrap_samples(X, y, min_samples=300):\n",
    "    n_samples=X.shape[0]\n",
    "\n",
    "    min_samples=n_samples if min_samples is None else min_samples\n",
    "\n",
    "    idxs = np.random.choice(n_samples, min_samples, replace=True)\n",
    "\n",
    "    return X[idxs, :], y[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3c54b3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n",
    "\"\"\"\n",
    "Random Forest is an ensemble learning method for classification and regression that operates\n",
    "by constructing a collection of decision trees, where each tree is trained on a randomly selected subset \n",
    "of the training data. The predictions made by the individual trees are then combined, \n",
    "either by voting in classification or by averaging in regression, to produce a single prediction.\n",
    "\"\"\"\n",
    "\n",
    "def RandomForestClassifier(X, y, n_estimators=100):\n",
    "    n_trees = []\n",
    "    for _ in range(n_estimators):\n",
    "        X, y = _bootstrap_samples(X, y)\n",
    "        m = build_tree(X, y)\n",
    "        n_trees.append(m)\n",
    "    return n_trees\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(X_train, y_train, n_estimators=100)\n",
    "\n",
    "\n",
    "def predict(X, tree):\n",
    "    predicted = np.array([_traverse(x, tree) for x in X])\n",
    "    return predicted\n",
    "\n",
    "\n",
    "\n",
    "def _traverse(X, tree):\n",
    "    if tree.value is not None:\n",
    "        return tree.value\n",
    "\n",
    "    if X[tree.feature]<=tree.threshold:\n",
    "        return _traverse(X, tree.left)\n",
    "\n",
    "    return _traverse(X, tree.right)\n",
    "\n",
    "\n",
    "def _majority_voting(X):\n",
    "    return mode(X, axis=1)[0]\n",
    "\n",
    "\n",
    "def rf_predict(X):\n",
    "    preds = [predict(X_test, m) for m in model]\n",
    "    predicted = np.stack(preds, axis=1)\n",
    "    predicted = _majority_voting(predicted)\n",
    "    return predicted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "66f1dacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.75524475524476"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = rf_predict(X_test)\n",
    "\n",
    "np.sum(predictions==y_test)/y_test.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
